#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
WZé¡¹ç›®é›†æˆä¸»ç¨‹åº
æä¾›ç»Ÿä¸€çš„å‘½ä»¤è¡Œæ¥å£ï¼Œæ•´åˆæ‰€æœ‰æ¨¡å—åŠŸèƒ½
"""

import sys
import json
import argparse
import asyncio
from pathlib import Path
from typing import List, Dict, Any

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from core.integrated_crawler import IntegratedCrawler, batch_crawl_from_database, crawl_urls
from core.database import UnifiedDatabaseManager, get_db_manager
from core.config import get_config, set_config_file
from wzzq.wechat_crawler import WechatCrawler
from wechat_mp_auth.auth import WeChatAuth

def setup_logging(verbose: bool = False):
    """è®¾ç½®æ—¥å¿—"""
    import logging
    
    level = logging.DEBUG if verbose else logging.INFO
    format_str = '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    
    logging.basicConfig(
        level=level,
        format=format_str,
        handlers=[
            logging.StreamHandler(),
            logging.FileHandler('wz_integrated.log', encoding='utf-8')
        ]
    )

def cmd_crawl_database(args):
    """ä»æ•°æ®åº“é‡‡é›†æ–‡ç« å‘½ä»¤"""
    print("=== ä»æ•°æ®åº“é‡‡é›†æ–‡ç«  ===")
    
    result = batch_crawl_from_database(
        source_type=args.source_type,
        limit=args.limit,
        batch_size=args.batch_size
    )
    
    print(f"é‡‡é›†å®Œæˆ:")
    print(f"  æ€»è®¡å¤„ç†: {result.get('total_processed', 0)}")
    print(f"  æˆåŠŸ: {result.get('successful', 0)}")
    print(f"  å¤±è´¥: {result.get('failed', 0)}")
    print(f"  è·³è¿‡: {result.get('skipped', 0)}")
    
    if args.output:
        with open(args.output, 'w', encoding='utf-8') as f:
            json.dump(result, f, indent=2, ensure_ascii=False, default=str)
        print(f"ç»“æœå·²ä¿å­˜åˆ°: {args.output}")

def cmd_crawl_urls(args):
    """æ ¹æ®URLåˆ—è¡¨é‡‡é›†å‘½ä»¤"""
    print("=== æ ¹æ®URLåˆ—è¡¨é‡‡é›† ===")
    
    urls = []
    if args.urls:
        urls.extend(args.urls)
    
    if args.url_file:
        with open(args.url_file, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if line and not line.startswith('#'):
                    urls.append(line)
    
    if not urls:
        print("é”™è¯¯: æ²¡æœ‰æä¾›URL")
        return
    
    print(f"å‡†å¤‡é‡‡é›† {len(urls)} ä¸ªURL")
    
    result = crawl_urls(
        urls=urls,
        source_type=args.source_type or "external",
        source_name=args.source_name or "æ‰‹åŠ¨å¯¼å…¥"
    )
    
    print(f"é‡‡é›†å®Œæˆ:")
    print(f"  æ€»è®¡: {result['total']}")
    print(f"  æˆåŠŸ: {result['summary']['success']}")
    print(f"  å¤±è´¥: {result['summary']['failed']}")
    print(f"  è·³è¿‡: {result['summary']['skipped']}")
    print(f"  é”™è¯¯: {result['summary']['error']}")
    
    if args.output:
        with open(args.output, 'w', encoding='utf-8') as f:
            json.dump(result, f, indent=2, ensure_ascii=False, default=str)
        print(f"ç»“æœå·²ä¿å­˜åˆ°: {args.output}")

def cmd_fetch_wechat_links(args):
    """è·å–å¾®ä¿¡å…¬ä¼—å·é“¾æ¥å‘½ä»¤"""
    print("=== è·å–å¾®ä¿¡å…¬ä¼—å·é“¾æ¥ ===")
    
    try:
        # åˆå§‹åŒ–å¾®ä¿¡è®¤è¯
        auth = WeChatAuth()
        if not auth.check_login_status():
            print("å¾®ä¿¡ç™»å½•çŠ¶æ€æ— æ•ˆï¼Œè¯·å…ˆç™»å½•")
            return
        
        # åˆå§‹åŒ–å¾®ä¿¡çˆ¬è™«
        crawler = WechatCrawler()
        
        # è·å–è´¦å·åˆ—è¡¨
        if args.account_name:
            accounts = [args.account_name]
        else:
            # ä»é…ç½®æ–‡ä»¶è·å–æ‰€æœ‰è´¦å·
            config = get_config()
            accounts_file = config.get_data_path(config.wechat.accounts_file)
            
            if accounts_file.exists():
                with open(accounts_file, 'r', encoding='utf-8') as f:
                    accounts_data = json.load(f)
                accounts = list(accounts_data.keys())
            else:
                print("é”™è¯¯: æœªæ‰¾åˆ°è´¦å·é…ç½®æ–‡ä»¶")
                return
        
        print(f"å‡†å¤‡è·å– {len(accounts)} ä¸ªè´¦å·çš„æ–‡ç« é“¾æ¥")
        
        total_fetched = 0
        for account in accounts:
            print(f"æ­£åœ¨å¤„ç†è´¦å·: {account}")
            try:
                count = crawler.crawl_account_articles(account, limit=args.limit_per_account)
                total_fetched += count
                print(f"  è·å–åˆ° {count} ç¯‡æ–‡ç« é“¾æ¥")
            except Exception as e:
                print(f"  å¤„ç†è´¦å·å¤±è´¥: {e}")
        
        print(f"æ€»è®¡è·å– {total_fetched} ç¯‡æ–‡ç« é“¾æ¥")
        
    except Exception as e:
        print(f"è·å–å¾®ä¿¡é“¾æ¥å¤±è´¥: {e}")

def cmd_status(args):
    """æŸ¥çœ‹ç³»ç»ŸçŠ¶æ€å‘½ä»¤"""
    print("=== WZç³»ç»ŸçŠ¶æ€ ===")
    
    try:
        db_manager = get_db_manager()
        if not db_manager.connect():
            print("âŒ æ•°æ®åº“è¿æ¥å¤±è´¥")
            return
        
        print("âœ… æ•°æ®åº“è¿æ¥æ­£å¸¸")
        
        # è·å–ç»Ÿè®¡ä¿¡æ¯
        crawl_stats = db_manager.get_crawl_statistics()
        
        print("\nğŸ“Š é‡‡é›†ç»Ÿè®¡:")
        for source_type, stats in crawl_stats.items():
            print(f"  {source_type}:")
            print(f"    æ€»è®¡: {stats['total_articles']}")
            print(f"    å·²å®Œæˆ: {stats['completed']}")
            print(f"    å¾…å¤„ç†: {stats['pending']}")
            print(f"    å¤±è´¥: {stats['failed']}")
            print(f"    å¹³å‡å­—æ•°: {stats['avg_word_count']:.0f}" if stats['avg_word_count'] else "    å¹³å‡å­—æ•°: 0")
        
        # æ£€æŸ¥é…ç½®
        config = get_config()
        print(f"\nâš™ï¸ é…ç½®çŠ¶æ€:")
        print(f"  å¾®ä¿¡é‡‡é›†: {'å¯ç”¨' if config.wechat.enabled else 'ç¦ç”¨'}")
        print(f"  CFCJé‡‡é›†: {'å¯ç”¨' if config.cfcj.enabled else 'ç¦ç”¨'}")
        print(f"  è‡ªåŠ¨å‘å¸ƒ: {'å¯ç”¨' if config.publisher.enabled else 'ç¦ç”¨'}")
        
        db_manager.disconnect()
        
    except Exception as e:
        print(f"è·å–çŠ¶æ€å¤±è´¥: {e}")

def cmd_config(args):
    """é…ç½®ç®¡ç†å‘½ä»¤"""
    print("=== é…ç½®ç®¡ç† ===")
    
    config = get_config()
    
    if args.show:
        print("å½“å‰é…ç½®:")
        config_dict = config.to_dict()
        print(json.dumps(config_dict, indent=2, ensure_ascii=False))
    
    elif args.set_key and args.set_value:
        if config.set(args.set_key, args.set_value):
            config.save_config()
            print(f"é…ç½®å·²æ›´æ–°: {args.set_key} = {args.set_value}")
        else:
            print(f"é…ç½®æ›´æ–°å¤±è´¥: {args.set_key}")
    
    elif args.get_key:
        value = config.get(args.get_key)
        print(f"{args.get_key} = {value}")
    
    else:
        print("è¯·æŒ‡å®šé…ç½®æ“ä½œ (--show, --set-key/--set-value, --get-key)")

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='WZé¡¹ç›®é›†æˆç®¡ç†å·¥å…·')
    
    # å…¨å±€å‚æ•°
    parser.add_argument('--config', help='é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--verbose', '-v', action='store_true', help='è¯¦ç»†è¾“å‡º')
    parser.add_argument('--output', '-o', help='è¾“å‡ºæ–‡ä»¶è·¯å¾„')
    
    # å­å‘½ä»¤
    subparsers = parser.add_subparsers(dest='command', help='å¯ç”¨å‘½ä»¤')
    
    # ä»æ•°æ®åº“é‡‡é›†å‘½ä»¤
    crawl_db_parser = subparsers.add_parser('crawl-db', help='ä»æ•°æ®åº“é‡‡é›†æ–‡ç« ')
    crawl_db_parser.add_argument('--source-type', help='æºç±»å‹è¿‡æ»¤ (wechat, linux_do, nodeseek)')
    crawl_db_parser.add_argument('--limit', type=int, default=100, help='é™åˆ¶é‡‡é›†æ•°é‡')
    crawl_db_parser.add_argument('--batch-size', type=int, default=5, help='æ‰¹æ¬¡å¤§å°')
    
    # URLé‡‡é›†å‘½ä»¤
    crawl_urls_parser = subparsers.add_parser('crawl-urls', help='æ ¹æ®URLåˆ—è¡¨é‡‡é›†')
    crawl_urls_parser.add_argument('urls', nargs='*', help='URLåˆ—è¡¨')
    crawl_urls_parser.add_argument('--url-file', help='åŒ…å«URLçš„æ–‡ä»¶')
    crawl_urls_parser.add_argument('--source-type', default='external', help='æºç±»å‹')
    crawl_urls_parser.add_argument('--source-name', default='æ‰‹åŠ¨å¯¼å…¥', help='æºåç§°')
    
    # å¾®ä¿¡é“¾æ¥è·å–å‘½ä»¤
    wechat_parser = subparsers.add_parser('fetch-wechat', help='è·å–å¾®ä¿¡å…¬ä¼—å·é“¾æ¥')
    wechat_parser.add_argument('--account-name', help='æŒ‡å®šè´¦å·åç§°')
    wechat_parser.add_argument('--limit-per-account', type=int, default=20, help='æ¯ä¸ªè´¦å·çš„æ–‡ç« æ•°é‡é™åˆ¶')
    
    # çŠ¶æ€æŸ¥çœ‹å‘½ä»¤
    status_parser = subparsers.add_parser('status', help='æŸ¥çœ‹ç³»ç»ŸçŠ¶æ€')
    
    # é…ç½®ç®¡ç†å‘½ä»¤
    config_parser = subparsers.add_parser('config', help='é…ç½®ç®¡ç†')
    config_parser.add_argument('--show', action='store_true', help='æ˜¾ç¤ºå½“å‰é…ç½®')
    config_parser.add_argument('--get-key', help='è·å–é…ç½®å€¼')
    config_parser.add_argument('--set-key', help='è®¾ç½®é…ç½®é”®')
    config_parser.add_argument('--set-value', help='è®¾ç½®é…ç½®å€¼')
    
    # è§£æå‚æ•°
    args = parser.parse_args()
    
    # è®¾ç½®æ—¥å¿—
    setup_logging(args.verbose)
    
    # è®¾ç½®é…ç½®æ–‡ä»¶
    if args.config:
        set_config_file(args.config)
    
    # æ‰§è¡Œå‘½ä»¤
    if args.command == 'crawl-db':
        cmd_crawl_database(args)
    elif args.command == 'crawl-urls':
        cmd_crawl_urls(args)
    elif args.command == 'fetch-wechat':
        cmd_fetch_wechat_links(args)
    elif args.command == 'status':
        cmd_status(args)
    elif args.command == 'config':
        cmd_config(args)
    else:
        parser.print_help()

if __name__ == "__main__":
    main()
