#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¾®ä¿¡å…¬ä¼—å·å†…å®¹æå–æµ‹è¯•è„šæœ¬
ä½¿ç”¨ trafilatura + newspaper3k è¿›è¡Œæ™ºèƒ½å†…å®¹æå–
"""

import requests
import logging
from typing import Dict, Any, Optional

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def extract_with_trafilatura(url: str) -> Dict[str, Any]:
    """ä½¿ç”¨ trafilatura æå–å†…å®¹"""
    try:
        import trafilatura
        
        # ä¸‹è½½ç½‘é¡µå†…å®¹
        downloaded = trafilatura.fetch_url(url)
        if not downloaded:
            return {'success': False, 'error': 'æ— æ³•ä¸‹è½½ç½‘é¡µå†…å®¹'}
        
        # æå–ä¸»è¦å†…å®¹
        result = trafilatura.extract(
            downloaded,
            include_comments=False,  # ä¸åŒ…å«è¯„è®º
            include_tables=True,     # åŒ…å«è¡¨æ ¼
            include_images=True,     # åŒ…å«å›¾ç‰‡ä¿¡æ¯
            include_links=True,      # åŒ…å«é“¾æ¥
            output_format='txt'      # è¾“å‡ºçº¯æ–‡æœ¬
        )
        
        if not result:
            return {'success': False, 'error': 'trafilatura æ— æ³•æå–å†…å®¹'}
        
        # è·å–å…ƒæ•°æ®
        metadata = trafilatura.extract_metadata(downloaded)
        
        return {
            'success': True,
            'method': 'trafilatura',
            'title': metadata.title if metadata else '',
            'author': metadata.author if metadata else '',
            'date': metadata.date if metadata else '',
            'content': result,
            'word_count': len(result),
            'url': url
        }
        
    except ImportError:
        return {'success': False, 'error': 'trafilatura æœªå®‰è£…'}
    except Exception as e:
        return {'success': False, 'error': f'trafilatura æå–å¤±è´¥: {str(e)}'}

def extract_with_newspaper(url: str) -> Dict[str, Any]:
    """ä½¿ç”¨ newspaper3k æå–å†…å®¹"""
    try:
        from newspaper import Article
        
        # åˆ›å»ºæ–‡ç« å¯¹è±¡
        article = Article(url, language='zh')
        
        # ä¸‹è½½å’Œè§£æ
        article.download()
        article.parse()
        
        # å°è¯•æå–å…³é”®è¯å’Œæ‘˜è¦ï¼ˆå¯èƒ½éœ€è¦é¢å¤–çš„NLPå¤„ç†ï¼‰
        try:
            article.nlp()
        except:
            pass  # NLPå¤„ç†å¤±è´¥ä¸å½±å“ä¸»è¦å†…å®¹æå–
        
        if not article.text:
            return {'success': False, 'error': 'newspaper3k æ— æ³•æå–å†…å®¹'}
        
        return {
            'success': True,
            'method': 'newspaper3k',
            'title': article.title or '',
            'author': ', '.join(article.authors) if article.authors else '',
            'date': article.publish_date.isoformat() if article.publish_date else '',
            'content': article.text,
            'word_count': len(article.text),
            'summary': article.summary if hasattr(article, 'summary') else '',
            'keywords': article.keywords if hasattr(article, 'keywords') else [],
            'images': list(article.images) if article.images else [],
            'url': url
        }
        
    except ImportError:
        return {'success': False, 'error': 'newspaper3k æœªå®‰è£…'}
    except Exception as e:
        return {'success': False, 'error': f'newspaper3k æå–å¤±è´¥: {str(e)}'}

def extract_with_both(url: str) -> Dict[str, Any]:
    """ä½¿ç”¨ä¸¤ç§æ–¹æ³•æå–å†…å®¹å¹¶æ¯”è¾ƒç»“æœ"""
    logger.info(f"å¼€å§‹æå–URL: {url}")
    
    # ä½¿ç”¨ä¸¤ç§æ–¹æ³•æå–
    trafilatura_result = extract_with_trafilatura(url)
    newspaper_result = extract_with_newspaper(url)
    
    # åˆ†æç»“æœ
    results = {
        'url': url,
        'trafilatura': trafilatura_result,
        'newspaper': newspaper_result,
        'comparison': {}
    }
    
    # æ¯”è¾ƒä¸¤ç§æ–¹æ³•çš„æ•ˆæœ
    if trafilatura_result['success'] and newspaper_result['success']:
        traf_content = trafilatura_result['content']
        news_content = newspaper_result['content']
        
        results['comparison'] = {
            'trafilatura_length': len(traf_content),
            'newspaper_length': len(news_content),
            'length_ratio': len(traf_content) / len(news_content) if len(news_content) > 0 else 0,
            'recommended': 'trafilatura' if len(traf_content) > len(news_content) else 'newspaper'
        }
        
        # é€‰æ‹©æ›´å¥½çš„ç»“æœ
        if len(traf_content) > len(news_content):
            results['best_result'] = trafilatura_result
        else:
            results['best_result'] = newspaper_result
    elif trafilatura_result['success']:
        results['best_result'] = trafilatura_result
        results['comparison']['recommended'] = 'trafilatura'
    elif newspaper_result['success']:
        results['best_result'] = newspaper_result
        results['comparison']['recommended'] = 'newspaper'
    else:
        results['best_result'] = {'success': False, 'error': 'ä¸¤ç§æ–¹æ³•éƒ½å¤±è´¥äº†'}
    
    return results

def clean_wechat_content(content: str) -> str:
    """æ¸…ç†å¾®ä¿¡æ–‡ç« å†…å®¹çš„è¾…åŠ©å‡½æ•°"""
    if not content:
        return content
    
    # å¸¸è§çš„å¾®ä¿¡æ–‡ç« æ— å…³å†…å®¹æ¨¡å¼
    unwanted_patterns = [
        'ç‚¹å‡»ä¸Šæ–¹è“å­—å…³æ³¨',
        'é•¿æŒ‰äºŒç»´ç å…³æ³¨',
        'æ‰«ç å…³æ³¨æˆ‘ä»¬',
        'ç‚¹å‡»é˜…è¯»åŸæ–‡',
        'åœ¨çœ‹ç‚¹è¿™é‡Œ',
        'åˆ†äº«ç‚¹è¿™é‡Œ',
        'ç‚¹å‡»ğŸ‘‡ä¸‹æ–¹å°å¡ç‰‡å…³æ³¨',
        'æ˜Ÿæ ‡ç½®é¡¶',
        'æ¨èé˜…è¯»',
        'å¾€æœŸç²¾å½©',
        'æ›´å¤šç²¾å½©å†…å®¹',
        'å…è´£å£°æ˜',
        'ç‰ˆæƒå£°æ˜',
        'å•†åŠ¡åˆä½œ',
        'æŠ•ç¨¿é‚®ç®±'
    ]
    
    lines = content.split('\n')
    cleaned_lines = []
    
    for line in lines:
        line = line.strip()
        if not line:
            continue
            
        # æ£€æŸ¥æ˜¯å¦åŒ…å«ä¸éœ€è¦çš„å†…å®¹
        should_skip = False
        for pattern in unwanted_patterns:
            if pattern in line:
                should_skip = True
                break
        
        if not should_skip:
            cleaned_lines.append(line)
    
    return '\n'.join(cleaned_lines)

def test_extraction(test_urls: list):
    """æµ‹è¯•å¤šä¸ªURLçš„æå–æ•ˆæœ"""
    print("=" * 80)
    print("å¾®ä¿¡å…¬ä¼—å·å†…å®¹æå–æµ‹è¯•")
    print("=" * 80)
    
    for i, url in enumerate(test_urls, 1):
        print(f"\nã€æµ‹è¯• {i}ã€‘URL: {url}")
        print("-" * 60)
        
        try:
            results = extract_with_both(url)
            
            if results['best_result']['success']:
                best = results['best_result']
                print(f"âœ… æå–æˆåŠŸ (æ¨èæ–¹æ³•: {results['comparison'].get('recommended', 'unknown')})")
                print(f"ğŸ“° æ ‡é¢˜: {best.get('title', 'æœªçŸ¥')}")
                print(f"ğŸ‘¤ ä½œè€…: {best.get('author', 'æœªçŸ¥')}")
                print(f"ğŸ“… æ—¥æœŸ: {best.get('date', 'æœªçŸ¥')}")
                print(f"ğŸ“ å­—æ•°: {best.get('word_count', 0)}")
                
                # æ˜¾ç¤ºå†…å®¹é¢„è§ˆï¼ˆå‰200å­—ç¬¦ï¼‰
                content = best.get('content', '')
                cleaned_content = clean_wechat_content(content)
                preview = cleaned_content[:200] + "..." if len(cleaned_content) > 200 else cleaned_content
                print(f"ğŸ“„ å†…å®¹é¢„è§ˆ:\n{preview}")
                
                # æ˜¾ç¤ºæ¸…ç†æ•ˆæœ
                if len(cleaned_content) != len(content):
                    reduction = len(content) - len(cleaned_content)
                    print(f"ğŸ§¹ æ¸…ç†æ•ˆæœ: å‡å°‘äº† {reduction} ä¸ªå­—ç¬¦ ({reduction/len(content)*100:.1f}%)")
                
            else:
                print(f"âŒ æå–å¤±è´¥: {results['best_result'].get('error', 'æœªçŸ¥é”™è¯¯')}")
                
        except Exception as e:
            print(f"âŒ æµ‹è¯•å¼‚å¸¸: {str(e)}")
        
        print("-" * 60)

if __name__ == "__main__":
    # æµ‹è¯•URLåˆ—è¡¨ï¼ˆè¯·æ›¿æ¢ä¸ºå®é™…çš„å¾®ä¿¡å…¬ä¼—å·æ–‡ç« URLï¼‰
    test_urls = [
        # ç¤ºä¾‹URLï¼Œè¯·æ›¿æ¢ä¸ºå®é™…çš„å¾®ä¿¡æ–‡ç« é“¾æ¥
        "https://mp.weixin.qq.com/s/example1",
        "https://mp.weixin.qq.com/s/example2",
    ]
    
    print("è¯·å°†æµ‹è¯•URLæ·»åŠ åˆ° test_urls åˆ—è¡¨ä¸­ï¼Œç„¶åè¿è¡Œæµ‹è¯•")
    print("ç¤ºä¾‹ç”¨æ³•:")
    print("python test_content_extraction.py")
    
    # å¦‚æœæœ‰æµ‹è¯•URLï¼Œè¿è¡Œæµ‹è¯•
    if any("example" not in url for url in test_urls):
        test_extraction(test_urls)
    else:
        print("\nâš ï¸  è¯·å…ˆæ·»åŠ çœŸå®çš„å¾®ä¿¡æ–‡ç« URLè¿›è¡Œæµ‹è¯•")
