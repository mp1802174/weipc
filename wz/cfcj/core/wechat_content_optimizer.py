#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¾®ä¿¡å…¬ä¼—å·å†…å®¹ä¼˜åŒ–å™¨
ä½¿ç”¨ trafilatura + newspaper3k è¿›è¡Œæ™ºèƒ½å†…å®¹æå–å’Œæ¸…ç†
"""

import re
import logging
from typing import Dict, Any, Optional, List
from urllib.parse import urlparse

logger = logging.getLogger(__name__)

class WeChatContentOptimizer:
    """å¾®ä¿¡å…¬ä¼—å·å†…å®¹ä¼˜åŒ–å™¨"""
    
    def __init__(self):
        self.unwanted_patterns = [
            # å…³æ³¨ç›¸å…³
            r'ç‚¹å‡».*?å…³æ³¨',
            r'é•¿æŒ‰.*?å…³æ³¨',
            r'æ‰«ç å…³æ³¨',
            r'å…³æ³¨.*?å…¬ä¼—å·',
            r'ç‚¹å‡»ä¸Šæ–¹.*?å…³æ³¨',
            r'ç‚¹å‡»ğŸ‘‡.*?å…³æ³¨',
            r'æ˜Ÿæ ‡ç½®é¡¶',
            
            # äº’åŠ¨ç›¸å…³
            r'ç‚¹å‡».*?é˜…è¯»åŸæ–‡',
            r'åœ¨çœ‹ç‚¹è¿™é‡Œ',
            r'åˆ†äº«ç‚¹è¿™é‡Œ',
            r'ç‚¹èµ.*?åœ¨çœ‹',
            r'è½¬å‘.*?æœ‹å‹åœˆ',
            
            # æ¨å¹¿ç›¸å…³
            r'æ¨èé˜…è¯»',
            r'å¾€æœŸç²¾å½©',
            r'æ›´å¤šç²¾å½©å†…å®¹',
            r'çƒ­é—¨æ–‡ç« ',
            r'ç›¸å…³é˜…è¯»',
            
            # ç‰ˆæƒç›¸å…³
            r'å…è´£å£°æ˜',
            r'ç‰ˆæƒå£°æ˜',
            r'ç‰ˆæƒæ‰€æœ‰',
            r'è½¬è½½è¯·æ³¨æ˜',
            
            # å•†åŠ¡ç›¸å…³
            r'å•†åŠ¡åˆä½œ',
            r'æŠ•ç¨¿é‚®ç®±',
            r'è”ç³»æˆ‘ä»¬',
            r'å¹¿å‘ŠæŠ•æ”¾',
            
            # ç‰¹æ®Šæ ‡è®°
            r'â€”â€”.*?èŠ‚é€‰è‡ª',
            r'æ¥æº[:ï¼š]',
            r'ç¼–è¾‘[:ï¼š]',
            r'å®¡æ ¸[:ï¼š]',
        ]
        
        # ç¼–è¯‘æ­£åˆ™è¡¨è¾¾å¼ä»¥æé«˜æ€§èƒ½
        self.compiled_patterns = [re.compile(pattern, re.IGNORECASE) for pattern in self.unwanted_patterns]
    
    def extract_with_trafilatura(self, url: str) -> Dict[str, Any]:
        """ä½¿ç”¨ trafilatura æå–å†…å®¹"""
        try:
            import trafilatura
            import requests

            logger.info(f"ä½¿ç”¨ trafilatura æå–: {url}")

            # å°è¯•ç›´æ¥ä½¿ç”¨requestsä¸‹è½½ï¼Œé¿å…ä»£ç†é—®é¢˜
            try:
                headers = {
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
                }
                response = requests.get(url, headers=headers, timeout=30)
                response.raise_for_status()
                downloaded = response.text
            except Exception as e:
                logger.warning(f"requestsä¸‹è½½å¤±è´¥ï¼Œå°è¯•trafilatura: {e}")
                # å›é€€åˆ°trafilaturaçš„ä¸‹è½½æ–¹æ³•
                downloaded = trafilatura.fetch_url(url)

            if not downloaded:
                return {'success': False, 'error': 'æ— æ³•ä¸‹è½½ç½‘é¡µå†…å®¹'}

            # æå–ä¸»è¦å†…å®¹
            content = trafilatura.extract(
                downloaded,
                include_comments=False,
                include_tables=True,
                include_images=False,  # å¾®ä¿¡æ–‡ç« å›¾ç‰‡å¤„ç†å¤æ‚ï¼Œæš‚æ—¶ä¸åŒ…å«
                include_links=False,   # å‡å°‘æ— å…³é“¾æ¥
                output_format='txt'
            )

            if not content:
                return {'success': False, 'error': 'trafilatura æ— æ³•æå–å†…å®¹'}

            # è·å–å…ƒæ•°æ®
            metadata = trafilatura.extract_metadata(downloaded)

            return {
                'success': True,
                'method': 'trafilatura',
                'title': metadata.title if metadata else '',
                'content': content,
                'word_count': len(content),
                'raw_length': len(downloaded) if downloaded else 0
            }

        except ImportError:
            return {'success': False, 'error': 'trafilatura æœªå®‰è£…'}
        except Exception as e:
            logger.error(f"trafilatura æå–å¤±è´¥: {e}")
            return {'success': False, 'error': f'trafilatura æå–å¤±è´¥: {str(e)}'}
    
    def extract_with_newspaper(self, url: str) -> Dict[str, Any]:
        """ä½¿ç”¨ newspaper3k æå–å†…å®¹"""
        try:
            from newspaper import Article
            import requests

            logger.info(f"ä½¿ç”¨ newspaper3k æå–: {url}")

            # åˆ›å»ºæ–‡ç« å¯¹è±¡ï¼Œé…ç½®è¯·æ±‚å‚æ•°
            article = Article(url, language='zh')

            # è®¾ç½®è¯·æ±‚å¤´
            article.config.browser_user_agent = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
            article.config.request_timeout = 30

            # ä¸‹è½½å’Œè§£æ
            article.download()

            # æ£€æŸ¥ä¸‹è½½æ˜¯å¦æˆåŠŸ
            if not article.html:
                return {'success': False, 'error': 'newspaper3k æ— æ³•ä¸‹è½½ç½‘é¡µ'}

            article.parse()

            if not article.text:
                return {'success': False, 'error': 'newspaper3k æ— æ³•æå–å†…å®¹'}

            return {
                'success': True,
                'method': 'newspaper3k',
                'title': article.title or '',
                'content': article.text,
                'word_count': len(article.text),
                'images': list(article.images) if article.images else []
            }

        except ImportError:
            return {'success': False, 'error': 'newspaper3k æœªå®‰è£…'}
        except Exception as e:
            logger.error(f"newspaper3k æå–å¤±è´¥: {e}")
            return {'success': False, 'error': f'newspaper3k æå–å¤±è´¥: {str(e)}'}
    
    def clean_wechat_content(self, content: str) -> str:
        """æ¸…ç†å¾®ä¿¡æ–‡ç« å†…å®¹"""
        if not content:
            return content
        
        logger.debug("å¼€å§‹æ¸…ç†å¾®ä¿¡æ–‡ç« å†…å®¹")
        
        # æŒ‰è¡Œåˆ†å‰²
        lines = content.split('\n')
        cleaned_lines = []
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
            
            # æ£€æŸ¥æ˜¯å¦åŒ¹é…ä¸éœ€è¦çš„æ¨¡å¼
            should_skip = False
            for pattern in self.compiled_patterns:
                if pattern.search(line):
                    logger.debug(f"è·³è¿‡è¡Œ: {line[:50]}...")
                    should_skip = True
                    break
            
            # è¿‡æ»¤è¿‡çŸ­çš„è¡Œï¼ˆå¯èƒ½æ˜¯æ— æ„ä¹‰çš„ç‰‡æ®µï¼‰
            if len(line) < 3:
                should_skip = True
            
            # è¿‡æ»¤çº¯ç¬¦å·è¡Œ
            if re.match(r'^[^\w\u4e00-\u9fff]*$', line):
                should_skip = True
            
            if not should_skip:
                cleaned_lines.append(line)
        
        cleaned_content = '\n'.join(cleaned_lines)
        
        # ç§»é™¤å¤šä½™çš„ç©ºè¡Œ
        cleaned_content = re.sub(r'\n{3,}', '\n\n', cleaned_content)
        
        reduction = len(content) - len(cleaned_content)
        if reduction > 0:
            logger.info(f"å†…å®¹æ¸…ç†å®Œæˆï¼Œå‡å°‘äº† {reduction} ä¸ªå­—ç¬¦ ({reduction/len(content)*100:.1f}%)")
        
        return cleaned_content.strip()
    
    def optimize_content(self, url: str) -> Dict[str, Any]:
        """ä¼˜åŒ–å¾®ä¿¡æ–‡ç« å†…å®¹æå–"""
        logger.info(f"å¼€å§‹ä¼˜åŒ–å†…å®¹æå–: {url}")
        
        # éªŒè¯æ˜¯å¦ä¸ºå¾®ä¿¡é“¾æ¥
        if 'mp.weixin.qq.com' not in url:
            logger.warning(f"éå¾®ä¿¡é“¾æ¥ï¼Œä½¿ç”¨æ ‡å‡†æå–: {url}")
            return self.extract_with_trafilatura(url)
        
        # ä½¿ç”¨ä¸¤ç§æ–¹æ³•æå–
        trafilatura_result = self.extract_with_trafilatura(url)
        newspaper_result = self.extract_with_newspaper(url)
        
        # é€‰æ‹©æœ€ä½³ç»“æœ
        best_result = self._select_best_result(trafilatura_result, newspaper_result)
        
        if best_result['success']:
            # æ¸…ç†å†…å®¹
            original_content = best_result['content']
            cleaned_content = self.clean_wechat_content(original_content)
            
            # æ›´æ–°ç»“æœ
            best_result['content'] = cleaned_content
            best_result['word_count'] = len(cleaned_content)
            best_result['original_word_count'] = len(original_content)
            best_result['cleaning_ratio'] = (len(original_content) - len(cleaned_content)) / len(original_content) if len(original_content) > 0 else 0
            
            logger.info(f"å†…å®¹ä¼˜åŒ–å®Œæˆ: {best_result['word_count']} å­—ç¬¦ (æ¸…ç†ç‡: {best_result['cleaning_ratio']*100:.1f}%)")
        
        return best_result
    
    def _select_best_result(self, trafilatura_result: Dict[str, Any], newspaper_result: Dict[str, Any]) -> Dict[str, Any]:
        """é€‰æ‹©æœ€ä½³æå–ç»“æœ"""
        
        # å¦‚æœåªæœ‰ä¸€ä¸ªæˆåŠŸï¼Œè¿”å›æˆåŠŸçš„é‚£ä¸ª
        if trafilatura_result['success'] and not newspaper_result['success']:
            logger.info("é€‰æ‹© trafilatura ç»“æœï¼ˆnewspaper å¤±è´¥ï¼‰")
            return trafilatura_result
        elif newspaper_result['success'] and not trafilatura_result['success']:
            logger.info("é€‰æ‹© newspaper ç»“æœï¼ˆtrafilatura å¤±è´¥ï¼‰")
            return newspaper_result
        elif not trafilatura_result['success'] and not newspaper_result['success']:
            logger.error("ä¸¤ç§æ–¹æ³•éƒ½å¤±è´¥äº†")
            return {'success': False, 'error': 'æ‰€æœ‰æå–æ–¹æ³•éƒ½å¤±è´¥äº†'}
        
        # ä¸¤ä¸ªéƒ½æˆåŠŸï¼Œæ¯”è¾ƒè´¨é‡
        traf_content = trafilatura_result['content']
        news_content = newspaper_result['content']
        
        traf_length = len(traf_content)
        news_length = len(news_content)
        
        # é€‰æ‹©ç­–ç•¥ï¼š
        # 1. ä¼˜å…ˆé€‰æ‹©å†…å®¹æ›´é•¿çš„ï¼ˆé€šå¸¸æ„å‘³ç€æå–æ›´å®Œæ•´ï¼‰
        # 2. å¦‚æœé•¿åº¦ç›¸è¿‘ï¼ˆå·®å¼‚<20%ï¼‰ï¼Œä¼˜å…ˆé€‰æ‹© trafilaturaï¼ˆé€šå¸¸æ›´å¹²å‡€ï¼‰
        
        if abs(traf_length - news_length) / max(traf_length, news_length) < 0.2:
            # é•¿åº¦ç›¸è¿‘ï¼Œä¼˜å…ˆé€‰æ‹© trafilatura
            logger.info(f"é€‰æ‹© trafilatura ç»“æœï¼ˆé•¿åº¦ç›¸è¿‘ï¼Œtrafilatura: {traf_length}, newspaper: {news_length}ï¼‰")
            return trafilatura_result
        elif traf_length > news_length:
            logger.info(f"é€‰æ‹© trafilatura ç»“æœï¼ˆå†…å®¹æ›´é•¿ï¼Œtrafilatura: {traf_length}, newspaper: {news_length}ï¼‰")
            return trafilatura_result
        else:
            logger.info(f"é€‰æ‹© newspaper ç»“æœï¼ˆå†…å®¹æ›´é•¿ï¼Œnewspaper: {news_length}, trafilatura: {traf_length}ï¼‰")
            return newspaper_result

# å…¨å±€å®ä¾‹
wechat_optimizer = WeChatContentOptimizer()

def optimize_wechat_content(url: str) -> Dict[str, Any]:
    """ä¼˜åŒ–å¾®ä¿¡å†…å®¹æå–çš„ä¾¿æ·å‡½æ•°"""
    return wechat_optimizer.optimize_content(url)
